{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:crewai.ipynb:logger is set up and ready to rock\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"crewai.ipynb\")\n",
    "\n",
    "logger.info(\"logger is set up and ready to rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crew Ai Sandbox\n",
    "\n",
    "## What is Crew AI?\n",
    "\n",
    "CrewAi is a framework to make it easier to work with roleplaying AI (LLM) agents to achieve a tree of thought. It is open source and its source code may be found on [GitHub](https://github.com/crewAIInc/crewAI)\n",
    "\n",
    "## Concepts involved\n",
    "\n",
    "* LLM (Large Language Models)\n",
    "* Prompt Engineering\n",
    "\n",
    "## Setting up\n",
    "\n",
    "I recommend using `virtualenv`, once you have it create a new `virtualenv` on this folder, active it (by `source`ing on Unix systems the `activate` script) and restore dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "You'll also want to have `ollama` running locally in order to pull and use local LLMs. In order to save money this sandbox uses local environments instead of expensive OpenAI calls. Ollama can be found at [their official website](https://ollama.com/download).\n",
    "\n",
    "To make it easier to interact with ollama without doing manual steps in the terminal or gui we'll be using `ollama-python` to set things up using the notebook itself.\n",
    "\n",
    "### Problems with Python 3.13\n",
    "\n",
    "If you are using Python 3.13 you might run into issues (as of 2024-12-14) when installing CrewAi thru pip.\n",
    "\n",
    "In order to get my environment to work I had to:\n",
    "\n",
    "1. Set up my machine to run `rust` by using the script from [RustUp.rs](https://rustup.rs)\n",
    "2. Add rust's cargo tools to my path by sourcing it on `.zshrc`\n",
    "3. Exporting a specific variable in order to toggle compatibility between 3.12 and 3.13 when compiling: `export PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1`\n",
    "\n",
    "## Experimenting\n",
    "\n",
    "### Ensuring that ollama is available in the environment\n",
    "\n",
    "Before committing to a solution I would like to ensure that it is possible to have things set in a way that they are repeatable so, I'll begin this experiment by looking at the `ollama` library and if (using it) we can tell whenever ollama is or isn't running.\n",
    "\n",
    "From looking at their docs seems like the simplest way to ensure ollama is available is to try and `list` models as, even if there are no models available, the result is non-error. That way if there is an exception it means that somehow `ollama` either isn't available or isn't set up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:crewai.ipynb:ensuring that ollama is up and running\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "INFO:crewai.ipynb:ollama is up and running\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"ensuring that ollama is up and running\")\n",
    "from ollama import list\n",
    "\n",
    "try:\n",
    "    list()\n",
    "    logger.info(\"ollama is up and running\")\n",
    "except:\n",
    "    logger.error(\"ollama is not running, exiting\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know it's available we can go ahead and pull from one model we know it's available. As of the time of this experiment the model I'm most aware of is `llama3` so I'll use it as the desired one.\n",
    "\n",
    "An extensive list of available models may be found on [ollama's website](https://ollama.com/library).\n",
    "\n",
    "Once a model is pulled and available locally we can prompt it, without creating a chat, by using the `generate` function and passing in a prompt we`d like to ask it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:crewai.ipynb:model llama3 found locally, no need to pull\n",
      "INFO:crewai.ipynb:asking the model why the sky is blue\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:crewai.ipynb:model's response: The sky appears blue because of a phenomenon called Rayleigh scattering. Short-wavelength blue light is scattered more by tiny molecules of gases like nitrogen and oxygen, making it reach our eyes from all directions. This scattering effect dominates over other wavelengths, giving us the blue color!\n"
     ]
    }
   ],
   "source": [
    "from ollama import pull, show, generate\n",
    "DESIRED_MODEL = \"llama3\"\n",
    "\n",
    "# attempt to show the model, if it doesn't exist it means we need to pull it\n",
    "try:\n",
    "    show(DESIRED_MODEL)\n",
    "    logger.info(f\"model {DESIRED_MODEL} found locally, no need to pull\")\n",
    "except:\n",
    "    logger.info(f\"model {DESIRED_MODEL} not found, pulling it\")\n",
    "    pull(DESIRED_MODEL)\n",
    "    logger.info(f\"model {DESIRED_MODEL} pulled successfully\")\n",
    "\n",
    "logger.info(\"asking the model why the sky is blue\")\n",
    "response = generate(DESIRED_MODEL, \"why is the sky blue, in up to 256 characters\")\n",
    "logger.info(f\"model's response: {response.response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
