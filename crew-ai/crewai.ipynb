{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:crewai.ipynb:logger is set up and ready to rock\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"crewai.ipynb\")\n",
    "\n",
    "logger.info(\"logger is set up and ready to rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crew Ai Sandboc\n",
    "\n",
    "## What is Crew AI?\n",
    "\n",
    "CrewAi is a framework to make it easier to work with roleplaying AI (LLM) agents to achieve a tree of thought. It is open source and its source code may be found on [GitHub](https://github.com/crewAIInc/crewAI)\n",
    "\n",
    "## Concepts involved\n",
    "\n",
    "* LLM (Large Language Models)\n",
    "* Prompt Engineering\n",
    "\n",
    "## Setting up\n",
    "\n",
    "I recommend using `virtualenv`, once you have it create a new `virtualenv` on this folder, active it (by `source`ing on Unix systems the `activate` script) and restore dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "You'll also want to have `ollama` running locally in order to pull and use local LLMs. In order to save money this sandbox uses local environments instead of expensive OpenAI calls. Ollama can be found at [their official website](https://ollama.com/download).\n",
    "\n",
    "To make it easier to interact with ollama without doing manual steps in the terminal or gui we'll be using `ollama-python` to set things up using the notebook itself.\n",
    "\n",
    "### Problems with Python 3.13\n",
    "\n",
    "If you are using Python 3.13 you might run into issues (as of 2024-12-14) when installing CrewAi thru pip.\n",
    "\n",
    "In order to get my environment to work I had to:\n",
    "\n",
    "1. Set up my machine to run `rust` by using the script from [RustUp.rs](https://rustup.rs)\n",
    "2. Add rust's cargo tools to my path by sourcing it on `.zshrc`\n",
    "3. Exporting a specific variable in order to toggle compatibility between 3.12 and 3.13 when compiling: `export PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:crewai.ipynb:ensuring that ollama is up and running\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "INFO:crewai.ipynb:ollama is up and running\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:crewai.ipynb:model llama3 found locally, no need to pull\n",
      "INFO:crewai.ipynb:asking the model why the sky is blue\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:crewai.ipynb:model response: The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, shorter blue wavelengths scatter more than longer red wavelengths due to interactions with tiny molecules like N2 and O2. This scattering effect gives the sky its blue color!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"ensuring that ollama is up and running\")\n",
    "from ollama import pull, show, generate, list\n",
    "\n",
    "try:\n",
    "    list()\n",
    "    logger.info(\"ollama is up and running\")\n",
    "except:\n",
    "    logger.error(\"ollama is not running, exiting\")\n",
    "    exit(1)\n",
    "\n",
    "DESIRED_MODEL = \"llama3\"\n",
    "\n",
    "# attempt to show the model, if it doesn't exist it means we need to pull it\n",
    "try:\n",
    "    show(DESIRED_MODEL)\n",
    "    logger.info(f\"model {DESIRED_MODEL} found locally, no need to pull\")\n",
    "except:\n",
    "    logger.info(f\"model {DESIRED_MODEL} not found, pulling it\")\n",
    "    pull(DESIRED_MODEL)\n",
    "    logger.info(f\"model {DESIRED_MODEL} pulled successfully\")\n",
    "\n",
    "logger.info(\"asking the model why the sky is blue\")\n",
    "response = generate(DESIRED_MODEL, \"why is the sky blue, in up to 256 characters\")\n",
    "logger.info(f\"model response: {response.response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
